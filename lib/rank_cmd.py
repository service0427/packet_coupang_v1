"""
ìƒí’ˆ ë“±ìˆ˜ ì²´í¬ ëª…ë ¹ì–´
"""

import sys
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from urllib.parse import quote

# Add lib directory to path
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent.parent / 'click-tracker'))

from common import (
    timestamp, get_cookie_by_id, get_latest_cookie, get_fingerprint,
    check_proxy_ip, parse_cookies, make_request, verify_ip_binding
)
from extractor.product_extractor import ProductExtractor
from traceid import generate_traceid
from realistic_click import realistic_click

BASE_DIR = Path(__file__).parent.parent

# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ìƒí’ˆ (í˜¸ë°• ë‹¬ë¹›ì‹í˜œ)
DEFAULT_PRODUCT = {
    'product_id': '9024146312',
    'item_id': '26462223016',
    'vendor_item_id': '93437504341',
    'query': 'í˜¸ë°• ë‹¬ë¹›ì‹í˜œ'
}

def fetch_page(page_num, query, trace_id, cookies, fingerprint, proxy):
    """í˜ì´ì§€ ê²€ìƒ‰"""
    url = f'https://www.coupang.com/np/search?q={quote(query)}&traceId={trace_id}&channel=user&listSize=72&page={page_num}'

    try:
        resp = make_request(url, cookies, fingerprint, proxy)
        size = len(resp.content)

        if resp.status_code == 200 and size > 5000:
            result = ProductExtractor.extract_products_from_html(resp.text)
            return {'page': page_num, 'success': True, 'products': result['ranking'], 'size': size}
        elif resp.status_code == 403:
            return {'page': page_num, 'success': False, 'products': [], 'error': 'BLOCKED_403'}
        elif size <= 5000:
            return {'page': page_num, 'success': False, 'products': [], 'error': f'CHALLENGE_{size}B'}
        else:
            return {'page': page_num, 'success': False, 'products': [], 'error': f'STATUS_{resp.status_code}'}

    except Exception as e:
        return {'page': page_num, 'success': False, 'products': [], 'error': str(e)[:50]}

def click_product(product_url, cookies, fingerprint, proxy, referer, verbose=True):
    """ìƒí’ˆ í´ë¦­ (ìƒì„¸ ì •ë³´ ì¶œë ¥)"""
    full_url = f'https://www.coupang.com{product_url}'

    try:
        resp = make_request(full_url, cookies, fingerprint, proxy, referer)
        size = len(resp.content)

        # ìƒì„¸ ì •ë³´ ì¶œë ¥
        if verbose:
            print(f"\n{'â”€' * 60}")
            print("ğŸ“¤ Request")
            print(f"{'â”€' * 60}")
            print(f"  URL: {full_url[:80]}...")
            print(f"  Method: GET")
            print(f"  Referer: {referer[:60]}..." if referer else "  Referer: None")

            # Request Headers (ì£¼ìš” í•­ëª©)
            req_headers = resp.request.headers if hasattr(resp, 'request') and hasattr(resp.request, 'headers') else {}
            if req_headers:
                print(f"  Headers:")
                for key in ['User-Agent', 'Accept', 'Sec-Ch-Ua', 'Sec-Fetch-Site']:
                    if key in req_headers:
                        val = req_headers[key]
                        if len(val) > 50:
                            val = val[:50] + '...'
                        print(f"    {key}: {val}")

            print(f"\n{'â”€' * 60}")
            print("ğŸ“¥ Response")
            print(f"{'â”€' * 60}")
            print(f"  Status: {resp.status_code}")
            print(f"  Size: {size:,} bytes")

            # Response Headers (ì£¼ìš” í•­ëª©)
            resp_headers = dict(resp.headers) if hasattr(resp, 'headers') else {}
            if resp_headers:
                print(f"  Headers:")
                for key in ['Content-Type', 'Content-Encoding', 'Server', 'X-Cache']:
                    if key.lower() in [k.lower() for k in resp_headers]:
                        actual_key = next(k for k in resp_headers if k.lower() == key.lower())
                        print(f"    {actual_key}: {resp_headers[actual_key]}")

            # ì¿ í‚¤ ì •ë³´
            if resp.cookies:
                print(f"  Set-Cookie: {len(resp.cookies)}ê°œ")

            print(f"{'â”€' * 60}\n")

        # ê²°ê³¼ íŒì •
        if size > 100000:
            return {
                'success': True,
                'status': resp.status_code,
                'size': size,
                'url': full_url,
                'response_headers': dict(resp.headers) if hasattr(resp, 'headers') else {}
            }
        elif resp.status_code == 403:
            return {'success': False, 'status': 403, 'size': size, 'error': 'BLOCKED_403'}
        else:
            return {'success': False, 'status': resp.status_code, 'size': size, 'error': f'INVALID_RESPONSE_{size}B'}

    except Exception as e:
        return {'success': False, 'status': None, 'size': 0, 'error': str(e)[:100]}

def run_rank(args):
    """ìƒí’ˆ ë“±ìˆ˜ ì²´í¬ ì‹¤í–‰"""
    print("=" * 70)
    print("ìƒí’ˆ ë“±ìˆ˜ ì²´ì»¤")
    print("=" * 70)

    # Get cookie
    if args.cookie_id:
        cookie_record = get_cookie_by_id(args.cookie_id)
        if not cookie_record:
            print(f"âŒ ì¿ í‚¤ ID {args.cookie_id} ì—†ìŒ")
            return
    else:
        cookie_record = get_latest_cookie()
        if not cookie_record:
            print("âŒ ì¿ í‚¤ ì—†ìŒ")
            return
        print(f"ì¿ í‚¤ ID: {cookie_record['id']} (ìµœì‹ )")

    fingerprint = get_fingerprint(cookie_record['chrome_version'], args.platform)
    if not fingerprint:
        print("âŒ í•‘ê±°í”„ë¦°íŠ¸ ì—†ìŒ")
        return

    cookies = parse_cookies(cookie_record)
    # DBì—ëŠ” host:portë§Œ ì €ì¥, ì‚¬ìš©ì‹œ socks5:// ì¶”ê°€
    db_proxy = cookie_record['proxy_url']
    proxy = args.proxy or (f'socks5://{db_proxy}' if db_proxy else None)

    print(f"íƒ€ê²Ÿ: {args.product_id}")
    print(f"ê²€ìƒ‰ì–´: {args.query}")
    print(f"Chrome: {cookie_record['chrome_version']}")
    print(f"í”„ë¡ì‹œ: {proxy}")

    # IP ê²€ì¦
    if proxy:
        print("\nIP ê²€ì¦ ì¤‘...")
        success, current_ip, message = verify_ip_binding(proxy, cookie_record['proxy_ip'])

        if current_ip:
            print(f"  í˜„ì¬ IP: {current_ip}")
            print(f"  ì¿ í‚¤ IP: {cookie_record['proxy_ip']}")

        if not success:
            print(f"\nâŒ {message}")
            return

        print("  âœ… IP ì¼ì¹˜")

    print("=" * 70)

    trace_id = generate_traceid()
    print(f"\n[{timestamp()}] ê²€ìƒ‰ ì¤‘... (traceId: {trace_id})")

    found = None
    start_time = datetime.now()
    all_products = []
    failed_pages = []

    with ThreadPoolExecutor(max_workers=args.max_page) as executor:
        futures = {
            executor.submit(fetch_page, i, args.query, trace_id, cookies, fingerprint, proxy): i
            for i in range(1, args.max_page + 1)
        }

        for future in as_completed(futures, timeout=60):
            try:
                result = future.result(timeout=30)

                if result['success']:
                    for product in result['products']:
                        product['_page'] = result['page']
                        all_products.append(product)

                        if product['productId'] == args.product_id and not found:
                            found = product
                            found['page'] = result['page']
                            print(f"[{timestamp()}] âœ… ë°œê²¬! Page {result['page']}, Rank {product['rank']}")

                    if found:
                        # ì¡°ê¸° ì¢…ë£Œ: ëŒ€ê¸° ì¤‘ì¸ ì‘ì—… ì·¨ì†Œ
                        for f in futures:
                            f.cancel()
                        break
                    else:
                        print(f"  [{timestamp()}] Page {result['page']:2d}: {len(result['products'])}ê°œ")
                else:
                    print(f"  [{timestamp()}] Page {result['page']:2d}: âŒ {result.get('error')}")
                    failed_pages.append(result['page'])

            except:
                pass

        # ì¬ì‹œë„ (ë°œê²¬ ì•ˆ ëœ ê²½ìš°ë§Œ)
        if not found and failed_pages:
            print(f"\n[{timestamp()}] ì¬ì‹œë„: {failed_pages}")
            retry_futures = {
                executor.submit(fetch_page, p, args.query, trace_id, cookies, fingerprint, proxy): p
                for p in failed_pages
            }

            for future in as_completed(retry_futures, timeout=60):
                try:
                    result = future.result(timeout=30)
                    if result['success']:
                        for product in result['products']:
                            product['_page'] = result['page']
                            all_products.append(product)
                            if product['productId'] == args.product_id:
                                found = product
                                found['page'] = result['page']
                                print(f"[{timestamp()}] âœ… ì¬ì‹œë„ ë°œê²¬! Page {result['page']}")
                        if found:
                            # ì¡°ê¸° ì¢…ë£Œ
                            for f in retry_futures:
                                f.cancel()
                            break
                except:
                    pass

    search_time = (datetime.now() - start_time).total_seconds()

    # ì‹¤ì œ ìˆœìœ„ ê³„ì‚° (í˜ì´ì§€ + URL rank ê¸°ì¤€ ì •ë ¬)
    all_products.sort(key=lambda p: (p['_page'], p.get('rank') or 999))
    for i, product in enumerate(all_products):
        product['actual_rank'] = i + 1

    # íƒ€ê²Ÿ ìƒí’ˆì˜ ì‹¤ì œ ìˆœìœ„ ì°¾ê¸°
    actual_rank = None
    if found:
        for product in all_products:
            if product['productId'] == args.product_id:
                actual_rank = product['actual_rank']
                break

    # ê²°ê³¼
    print("\n" + "=" * 70)
    print("ê²°ê³¼")
    print("=" * 70)

    report = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'cookie_id': cookie_record['id'],
        'chrome_version': cookie_record['chrome_version'],
        'proxy_ip': cookie_record['proxy_ip'],
        'query': args.query,
        'trace_id': trace_id,
        'target_product_id': args.product_id,
        'search_time': round(search_time, 2),
        'total_products': len(all_products),
        'list_size': 72,
    }

    if found:
        print(f"\nâœ… ìƒí’ˆ ë°œê²¬")
        print(f"   í˜ì´ì§€: {found['page']}")
        print(f"   URL rank: {found['rank']}")
        print(f"   ì‹¤ì œ ìˆœìœ„: {actual_rank}ë“± / {len(all_products)}ê°œ")
        print(f"   ìƒí’ˆëª…: {found['name'][:50]}...")
        print(f"\n   URL: https://www.coupang.com{found['url']}")

        report['found'] = True
        report['page'] = found['page']
        report['url_rank'] = found['rank']
        report['actual_rank'] = actual_rank
        report['product'] = {
            'productId': found['productId'],
            'name': found['name'],
            'url': found['url']
        }

        # ìƒí’ˆ í´ë¦­ (ê¸°ë³¸ ë™ì‘) - ì‹¤ì œ ë¸Œë¼ìš°ì € íŠ¸ë˜í”½ ì¬í˜„
        if not getattr(args, 'no_click', False):
            print(f"\n[{timestamp()}] ìƒí’ˆ í´ë¦­...")

            # URLì—ì„œ itemId, vendorItemId íŒŒì‹±
            from urllib.parse import urlparse, parse_qs
            parsed_url = urlparse(found['url'])
            params = parse_qs(parsed_url.query)

            product_info = {
                'productId': found['productId'],
                'itemId': params.get('itemId', [''])[0],
                'vendorItemId': params.get('vendorItemId', [''])[0],
                'url': found['url'],
                'rank': found['rank']
            }

            search_url = f'https://www.coupang.com/np/search?q={quote(args.query)}'
            click_result = realistic_click(product_info, search_url, cookies, fingerprint, proxy)

            if click_result['success']:
                page_result = click_result.get('product_page', {})
                print(f"âœ… í´ë¦­ ì„±ê³µ ({page_result.get('size', 0):,} bytes)")
            else:
                page_result = click_result.get('product_page', {})
                print(f"âŒ í´ë¦­ ì‹¤íŒ¨: {page_result.get('error', 'Unknown')}")

            report['click'] = click_result
    else:
        print(f"\nâŒ ìƒí’ˆ ë¯¸ë°œê²¬ ({args.max_page}í˜ì´ì§€)")
        report['found'] = False

    report['all_products'] = all_products

    # ë¦¬í¬íŠ¸ ì €ì¥
    reports_dir = BASE_DIR / 'reports'
    reports_dir.mkdir(exist_ok=True)
    filename = f"rank_{args.product_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    report_path = reports_dir / filename

    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ ë¦¬í¬íŠ¸: {report_path}")

    return report
