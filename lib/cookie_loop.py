"""
ì¿ í‚¤ ìƒì„± ë£¨í”„ - Playwright Python

ìƒì„± ëª¨ë“œ:
- basic: ë§¤ë²ˆ ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸ (ê¸°ë³¸)
- persistent: ìœ ì €í´ë” ì‚¬ìš© (ìºì‹œ/ìƒíƒœ ìœ ì§€)
"""

import os
import json
import random
import subprocess
from pathlib import Path
from urllib.parse import quote
from playwright.sync_api import sync_playwright
from db import insert_one, execute_query
from common import generate_traceid

# DISPLAY ì„¤ì •ì€ USE_XVFB í”Œë˜ê·¸ì— ë”°ë¼ generate_cookies_loopì—ì„œ ì²˜ë¦¬

CHROME_VERSIONS_DIR = Path(__file__).parent.parent / 'chrome-versions' / 'files'
USER_DATA_BASE_DIR = Path(__file__).parent.parent / 'chrome-profiles'

# ì¿ í‚¤ ìƒì„± ëª¨ë“œ ì„¤ì •
COOKIE_MODE = 'persistent'  # 'basic' ë˜ëŠ” 'persistent'

# íŠ¸ë˜í”½ ì°¨ë‹¨ ì„¤ì •
BLOCK_TRAFFIC = True  # True: íŠ¸ë˜í”½ ì ˆì•½ ëª¨ë“œ, False: ì „ì²´ ë¡œë“œ

# íƒ€ê²Ÿ URL ì„¤ì •
USE_SEARCH_PAGE = True  # True: ê²€ìƒ‰ í˜ì´ì§€, False: ë¡œê·¸ì¸ í˜ì´ì§€

# Xvfb ì‚¬ìš© ì„¤ì •
USE_XVFB = True  # True: ê°€ìƒ ë””ìŠ¤í”Œë ˆì´ (Xvfb), False: ì‹¤ì œ GUI

# ê²€ìƒ‰ í‚¤ì›Œë“œ (30ê°œ ë¡¤ë§)
SEARCH_KEYWORDS = [
    'ë…¸íŠ¸ë¶', 'ì•„ì´í°', 'ê°¤ëŸ­ì‹œ', 'ì—ì–´íŒŸ', 'ë§¥ë¶',
    'ì‚¼ì„±TV', 'LGëƒ‰ì¥ê³ ', 'ë‹¤ì´ìŠ¨', 'ë‚˜ì´í‚¤', 'ì•„ë””ë‹¤ìŠ¤',
    'ì‹ ë¼ë©´', 'ì½”ì¹´ì½œë¼', 'ì˜¤ë ˆì˜¤', 'ë°”ë‚˜ë‚˜ìš°ìœ ', 'ì´ˆì½”íŒŒì´',
    'ë¬´ì„ ì´ì–´í°', 'í‚¤ë³´ë“œ', 'ë§ˆìš°ìŠ¤', 'ëª¨ë‹ˆí„°', 'ì›¹ìº ',
    'ìš´ë™í™”', 'ë°±íŒ©', 'ì„ ê¸€ë¼ìŠ¤', 'ì‹œê³„', 'ì§€ê°‘',
    'ìƒ´í‘¸', 'ì¹˜ì•½', 'í™”ì¥ì§€', 'ì„¸ì œ', 'ë¬¼í‹°ìŠˆ'
]

def get_proxy_ip(proxy):
    """í”„ë¡ì‹œ ì™¸ë¶€ IP í™•ì¸"""
    if not proxy:
        return None
    try:
        result = subprocess.run(
            ['curl', '-s', '--proxy', proxy, 'https://api.ipify.org?format=json'],
            capture_output=True, text=True, timeout=10
        )
        return json.loads(result.stdout).get('ip')
    except:
        return None

def generate_cookies_loop(version, proxy, loop_count):
    """ì¿ í‚¤ ìƒì„± ë£¨í”„"""
    # Xvfb ì‚¬ìš© ì‹œ DISPLAY ì„¤ì •
    if USE_XVFB:
        os.environ['DISPLAY'] = ':99'

    # Find Chrome version
    chrome_dirs = [d for d in CHROME_VERSIONS_DIR.iterdir()
                   if d.is_dir() and version in d.name]

    if not chrome_dirs:
        print(f"âŒ Chrome ë²„ì „ {version} ì—†ìŒ")
        return []

    selected_dir = sorted(chrome_dirs)[-1]
    version_number = selected_dir.name.replace('chrome-', '')
    major_version = int(version_number.split('.')[0])

    # Find Chrome executable
    linux_path = selected_dir / 'chrome-linux64' / 'chrome'
    win_path = selected_dir / 'chrome-win64' / 'chrome.exe'
    chrome_path = linux_path if linux_path.exists() else win_path if win_path.exists() else None

    if not chrome_path:
        print("âŒ Chrome ì‹¤í–‰ íŒŒì¼ ì—†ìŒ")
        return []

    # Get proxy IP
    proxy_ip = get_proxy_ip(proxy)
    if proxy and not proxy_ip:
        print("âŒ í”„ë¡ì‹œ IP í™•ì¸ ì‹¤íŒ¨")
        return []

    print(f"Chrome: {version_number}")
    print(f"Proxy: {proxy or 'Direct'}")
    print(f"ì™¸ë¶€ IP: {proxy_ip or 'Direct'}")
    print(f"ëª¨ë“œ: {COOKIE_MODE}")
    print(f"ë°˜ë³µ: {loop_count}")
    print("â”€" * 50)

    results = []

    with sync_playwright() as p:
        # Launch options (coupang_agent_v2 ì„¤ì • ê¸°ë°˜)
        launch_args = [
            '--disable-blink-features=AutomationControlled',  # ìë™í™” ê°ì§€ ë°©ì§€
            '--test-type',                           # í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì¼ë¶€ ì œí•œ í•´ì œ)
            '--lang=ko-KR',                          # í•œêµ­ì–´ ì„¤ì •
            '--disable-translate',                   # ë²ˆì—­ íŒì—… ë°©ì§€
            '--disable-popup-blocking',              # íŒì—… ì°¨ë‹¨ ë¹„í™œì„±í™”
            '--no-sandbox',                          # ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™”
            '--disable-setuid-sandbox',              # Linux setuid ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™”
            '--disable-infobars',                    # ì •ë³´ ë°” ë¹„í™œì„±í™”
            '--enable-features=NetworkService,NetworkServiceInProcess',  # ë„¤íŠ¸ì›Œí¬ ì„œë¹„ìŠ¤
        ]

        if proxy:
            launch_args.append(f'--proxy-server={proxy}')

        # ëª¨ë“œì— ë”°ë¼ ë¸Œë¼ìš°ì € ì‹¤í–‰
        if COOKIE_MODE == 'persistent':
            # ìœ ì €í´ë” ë°©ì‹: ìºì‹œ/ìƒíƒœ ìœ ì§€
            user_data_dir = USER_DATA_BASE_DIR / f'chrome-{major_version}'
            user_data_dir.mkdir(parents=True, exist_ok=True)

            browser = p.chromium.launch_persistent_context(
                user_data_dir=str(user_data_dir),
                executable_path=str(chrome_path),
                headless=False,
                args=launch_args,
                ignore_default_args=['--enable-automation'],  # ìë™í™” íˆ´ë°” ìˆ¨ê¹€
                user_agent=f'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{major_version}.0.0.0 Safari/537.36'
            )
            context = browser
            is_persistent = True
        else:
            # ê¸°ë³¸ ë°©ì‹: ë§¤ë²ˆ ìƒˆë¡œìš´ ì»¨í…ìŠ¤íŠ¸
            browser = p.chromium.launch(
                executable_path=str(chrome_path),
                headless=False,
                args=launch_args,
                ignore_default_args=['--enable-automation']  # ìë™í™” íˆ´ë°” ìˆ¨ê¹€
            )
            context = browser.new_context(
                user_agent=f'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{major_version}.0.0.0 Safari/537.36'
            )
            is_persistent = False

        # persistent ëª¨ë“œ: ê¸°ì¡´ í˜ì´ì§€ ì‚¬ìš© (about:blank íƒ­), basic ëª¨ë“œ: ìƒˆ í˜ì´ì§€ ìƒì„±
        if is_persistent and context.pages:
            page = context.pages[0]
        else:
            page = context.new_page()

        # íŠ¸ë˜í”½ ì¸¡ì •
        total_bytes = [0]
        traffic_details = []

        # íŠ¸ë˜í”½ ì°¨ë‹¨ ëª¨ë“œ
        if BLOCK_TRAFFIC:
            def handle_route(route):
                resource_type = route.request.resource_type
                url = route.request.url

                # ì°¨ë‹¨ ëŒ€ìƒ: image, font, media, stylesheet
                if resource_type in ['image', 'font', 'media', 'stylesheet']:
                    route.abort()
                    return

                # ì™¸ë¶€ ì¶”ì /ê´‘ê³  ì°¨ë‹¨
                blocked_domains = [
                    'google-analytics.com', 'googletagmanager.com',
                    'facebook.net', 'doubleclick.net', 'criteo.com',
                    'amplitude.com', 'branch.io', 'appsflyer.com',
                    'ads.', 'track.', 'pixel.', 'beacon.',
                    'log.', 'analytics.', 'metrics.'
                ]
                if any(domain in url for domain in blocked_domains):
                    route.abort()
                    return

                # ì¿ íŒ¡ ë‚´ë¶€ ë¹„í•„ìˆ˜ API ì°¨ë‹¨
                blocked_paths = [
                    '/log/', '/beacon/', '/track/', '/pixel/',
                    '/recommend/', '/personalize/', '/ads/',
                    '/n-api/srp/omp-widget',
                    '/n-api/web-adapter/category-list',
                    '/n-api/srp/jikgu-promotion',
                    '/n-api/srp/brand-shop',
                    '/n-api/srp/top-brand'
                ]
                if any(path in url for path in blocked_paths):
                    route.abort()
                    return

                # ëŒ€ìš©ëŸ‰ JS chunk ì°¨ë‹¨
                if resource_type == 'script':
                    if 'coupangcdn.com' in url and '/chunks/' in url:
                        route.abort()
                        return

                route.continue_()

            page.route('**/*', handle_route)

        # ì‘ë‹µ íŠ¸ë˜í”½ ì¸¡ì •
        def on_response(response):
            try:
                body = response.body()
                size = len(body)
                total_bytes[0] += size
                if size > 100 * 1024:
                    traffic_details.append({
                        'url': response.url[:80],
                        'size': size
                    })
            except:
                pass

        page.on('response', on_response)


        # Loop
        for i in range(1, loop_count + 1):
            print(f"\n[{i}/{loop_count}] ì¿ í‚¤ ìƒì„± ì¤‘...")

            # Clear cookies
            context.clear_cookies()

            import time
            start_time = time.time()

            # 1ë‹¨ê³„: ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
            # page.goto('https://www.coupang.com/', wait_until='domcontentloaded', timeout=30000)
            # page.wait_for_timeout(1000)

            # 2ë‹¨ê³„: íƒ€ê²Ÿ í˜ì´ì§€ ì´ë™
            if USE_SEARCH_PAGE:
                keyword = random.choice(SEARCH_KEYWORDS)
                trace_id = generate_traceid()
                target_url = f'https://www.coupang.com/np/search?component=&q={quote(keyword)}&traceId={trace_id}&channel=user'
            else:
                target_url = 'https://login.coupang.com/login/login.pang'

            # coupang_agent_v2 ë°©ì‹: waitUntil='load' (ëª¨ë“  ë¦¬ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ)
            page.goto(target_url, wait_until='load', timeout=30000)

            # Access Denied ê°ì§€
            page_title = page.title()
            if 'Access Denied' in page_title or 'Error' in page_title:
                load_time = int((time.time() - start_time) * 1000)
                print(f"ğŸš« [{i}/{loop_count}] Access Denied - ê±´ë„ˆëœ€ | {load_time}ms")
                continue

            if USE_SEARCH_PAGE:
                # ê²€ìƒ‰ í˜ì´ì§€: ìƒí’ˆ ëª©ë¡ ëŒ€ê¸° (CSR ë Œë”ë§ ì™„ë£Œ)
                try:
                    page.wait_for_selector('#productList > li, #product-list > li', timeout=20000)
                except:
                    # íƒ€ì„ì•„ì›ƒ ì‹œ ì¶”ê°€ ëŒ€ê¸°
                    page.wait_for_timeout(5000)
            else:
                # ë¡œê·¸ì¸ í˜ì´ì§€ ë“±: ê¸°ë³¸ ëŒ€ê¸°
                page.wait_for_timeout(3000)

            load_time = int((time.time() - start_time) * 1000)

            # Get cookies
            cookies = context.cookies()

            # Extract Akamai cookies
            akamai_names = ['_abck', 'ak_bmsc', 'bm_sv', 'bm_sz', 'bm_s', 'bm_so', 'bm_ss', 'bm_lso']
            akamai_cookies = [c for c in cookies if c['name'] in akamai_names]

            abck = next((c for c in akamai_cookies if c['name'] == '_abck'), None)
            abck_value = abck['value'] if abck else None
            abck_valid = '~-1~' in abck_value if abck_value else False

            # ê²€ìƒ‰ í˜ì´ì§€ì¼ ë•Œë§Œ ìƒí’ˆ ê°œìˆ˜ ê²€ì¦
            product_count = 0
            if USE_SEARCH_PAGE:
                product_count = page.evaluate("""
                    () => {
                        const selectors = [
                            '#productList > li',
                            '#product-list > li[data-id]',
                            '#product-list > li',
                            'a[href*="/vp/products/"]'
                        ];
                        for (const sel of selectors) {
                            const count = document.querySelectorAll(sel).length;
                            if (count > 0) return count;
                        }
                        return 0;
                    }
                """)
                if product_count == 0:
                    print(f"âš ï¸ [{i}/{loop_count}] ì°¨ë‹¨ë¨ (ìƒí’ˆ 0ê°œ) - ê±´ë„ˆëœ€ | {load_time}ms")
                    continue

            # Skip invalid cookies
            if not abck_valid:
                print(f"âš ï¸ [{i}/{loop_count}] ìœ íš¨í•˜ì§€ ì•Šì€ ì¿ í‚¤ (_abck ì—†ìŒ) - ê±´ë„ˆëœ€ | {load_time}ms")
                continue

            # Save to database (only valid cookies)
            # proxy_urlì—ì„œ socks5:// ì œê±°í•˜ì—¬ host:portë§Œ ì €ì¥
            proxy_stripped = proxy.replace('socks5://', '') if proxy else None
            insert_id = insert_one("""
                INSERT INTO cookies
                (proxy_ip, proxy_url, chrome_version, cookie_data)
                VALUES (%s, %s, %s, %s)
            """, (
                proxy_ip or 'direct',
                proxy_stripped,
                version_number,
                json.dumps(cookies)
            ))

            # íŠ¸ë˜í”½ KB ê³„ì‚°
            traffic_kb = total_bytes[0] / 1024
            print(f"âœ… ID: {insert_id} | ìƒí’ˆ: {product_count}ê°œ | íŠ¸ë˜í”½: {traffic_kb:.0f}KB | ì¿ í‚¤: {len(cookies)}ê°œ | Akamai: {len(akamai_cookies)}ê°œ | {load_time}ms")

            # ëŒ€ìš©ëŸ‰ ìš”ì²­ ì¶œë ¥
            # if traffic_details:
            #     print(f"   ğŸ“¦ ëŒ€ìš©ëŸ‰ ìš”ì²­:")
            #     for t in sorted(traffic_details, key=lambda x: x['size'], reverse=True)[:5]:
            #         print(f"      {t['size']//1024}KB: {t['url']}")

            # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ë¦¬ì…‹
            total_bytes[0] = 0
            traffic_details.clear()

            results.append({
                'iteration': i,
                'cookie_id': insert_id,
                'cookie_count': len(cookies),
                'akamai_count': len(akamai_cookies),
                'load_time': load_time,
            })

            if i < loop_count:
                page.wait_for_timeout(1000)

        browser.close()

    # Summary
    print('\n' + 'â”€' * 50)
    print('ì™„ë£Œ')
    print('â”€' * 50)

    cookie_ids = [r['cookie_id'] for r in results]

    print(f"ì €ì¥: {len(results)}ê°œ")
    print(f"IDs: {', '.join(map(str, cookie_ids))}")

    return results

if __name__ == '__main__':
    import sys
    version = sys.argv[1] if len(sys.argv) > 1 else '136'
    loop_count = int(sys.argv[2]) if len(sys.argv) > 2 else 5
    proxy = None
    for arg in sys.argv[3:]:
        if arg.startswith('--proxy='):
            proxy = arg.split('=')[1]

    generate_cookies_loop(version, proxy, loop_count)
