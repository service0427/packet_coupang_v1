"""
ê²€ìƒ‰ ëª¨ë“ˆ - curl-cffi

Coupang ìƒí’ˆ ê²€ìƒ‰ ë° ìˆœìœ„ í™•ì¸
"""

from urllib.parse import quote
from concurrent.futures import ThreadPoolExecutor, as_completed

from work.request import make_request, parse_response_cookies, generate_trace_id, timestamp
from extractor.search_extractor import ProductExtractor


def fetch_page(page_num, query, trace_id, cookies, tls_profile, proxy, save_html=False, max_retries=2):
    """ë‹¨ì¼ í˜ì´ì§€ ê²€ìƒ‰ (TLS ì—ëŸ¬ ì‹œ ì¬ì‹œë„)

    Args:
        page_num: í˜ì´ì§€ ë²ˆí˜¸
        query: ê²€ìƒ‰ì–´
        trace_id: ì¿ íŒ¡ traceId
        cookies: ì¿ í‚¤ ë”•ì…”ë„ˆë¦¬
        tls_profile: TLS í”„ë¡œí•„ ë ˆì½”ë“œ (tls_profiles í…Œì´ë¸”)
        proxy: í”„ë¡ì‹œ URL
        save_html: HTML ì›ë³¸ ì €ì¥ ì—¬ë¶€
        max_retries: TLS ì—ëŸ¬ ì‹œ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸: 2)

    Returns:
        dict: {page, success, products, size, error, response_cookies, response_cookies_full, html, retried}
    """
    import time

    url = f'https://www.coupang.com/np/search?q={quote(query)}&traceId={trace_id}&channel=user&listSize=72&page={page_num}'

    # ì¬ì‹œë„ ëŒ€ìƒ ì—ëŸ¬ íŒ¨í„´
    # ë™ì¼ í”„ë¡ì‹œë¡œ ë‹¤ë¥¸ í˜ì´ì§€ê°€ ì„±ê³µí•˜ë©´ ì¼ì‹œì  ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¼ ìˆ˜ ìˆìŒ
    RETRYABLE_ERRORS = [
        'curl: (35)',           # TLS connect error
        'TLS connect error',
        'TLSV1_ALERT',
        'SSL routines',
        'curl: (56)',           # Recv failure
        'curl: (28)',           # Timeout
        'timed out',
        'curl: (7)',            # Connection refused
        'Could not connect',
        'curl: (92)',           # HTTP/2 stream error
        'HTTP/2 stream',
    ]

    last_error = None
    retried = 0

    for attempt in range(max_retries + 1):
        try:
            resp = make_request(url, cookies, tls_profile, proxy)
            size = len(resp.content)

            response_cookies, response_cookies_full = parse_response_cookies(resp)

            if resp.status_code == 200 and size > 5000:
                html_text = resp.text
                result = ProductExtractor.extract_products_from_html(html_text)

                # "ê²€ìƒ‰ê²°ê³¼ ì—†ìŒ" í˜ì´ì§€ ê°ì§€ (ì¿ íŒ¡ ì •ìƒ ì‘ë‹µì´ì§€ë§Œ ìƒí’ˆ ì—†ìŒ)
                is_no_results_page = (
                    'ì— ëŒ€í•œ ê²€ìƒ‰ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤' in html_text or
                    'ê²€ìƒ‰ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤' in html_text
                )

                return {
                    'page': page_num,
                    'success': True,
                    'products': result['ranking'],
                    'size': size,
                    'response_cookies': response_cookies,
                    'response_cookies_full': response_cookies_full,
                    'html': html_text if save_html else None,
                    'is_no_results_page': is_no_results_page,
                    'retried': retried
                }
            elif resp.status_code == 403:
                return {
                    'page': page_num,
                    'success': False,
                    'products': [],
                    'error': 'BLOCKED_403',
                    'response_cookies': response_cookies,
                    'response_cookies_full': response_cookies_full,
                    'html': None,
                    'retried': retried
                }
            elif size <= 5000:
                return {
                    'page': page_num,
                    'success': False,
                    'products': [],
                    'error': f'CHALLENGE_{size}B',
                    'response_cookies': response_cookies,
                    'response_cookies_full': response_cookies_full,
                    'html': None,
                    'retried': retried
                }
            else:
                return {
                    'page': page_num,
                    'success': False,
                    'products': [],
                    'error': f'STATUS_{resp.status_code}',
                    'response_cookies': response_cookies,
                    'response_cookies_full': response_cookies_full,
                    'html': None,
                    'retried': retried
                }

        except Exception as e:
            error_msg = str(e)[:150]
            last_error = error_msg

            # ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ì¸ì§€ í™•ì¸
            is_retryable = any(pattern in error_msg for pattern in RETRYABLE_ERRORS)

            if is_retryable and attempt < max_retries:
                retried += 1
                time.sleep(0.5)  # ì§§ì€ ëŒ€ê¸° í›„ ì¬ì‹œë„
                continue

            # ì¬ì‹œë„ ë¶ˆê°€ëŠ¥í•˜ê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ ë„ë‹¬
            return {
                'page': page_num,
                'success': False,
                'products': [],
                'error': error_msg,
                'response_cookies': {},
                'response_cookies_full': [],
                'html': None,
                'retried': retried
            }

    # max_retries ë„ë‹¬ (ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨)
    return {
        'page': page_num,
        'success': False,
        'products': [],
        'error': last_error or 'MAX_RETRIES',
        'response_cookies': {},
        'response_cookies_full': [],
        'html': None,
        'retried': retried
    }


def _match_product(product, target_product_id, target_item_id=None, target_vendor_item_id=None):
    """ìƒí’ˆ ë§¤ì¹­ ìš°ì„ ìˆœìœ„ ì²´í¬

    ìš°ì„ ìˆœìœ„:
    1. product_id + item_id + vendor_item_id (ì™„ì „ ë§¤ì¹­)
    2. product_id + vendor_item_id
    3. product_id + item_id
    4. product_idë§Œ
    5. vendor_item_idë§Œ
    6. item_idë§Œ

    Returns:
        tuple: (ë§¤ì¹­ ì—¬ë¶€, ë§¤ì¹­ íƒ€ì…) ë˜ëŠ” (False, None)
    """
    p_id = str(product.get('productId', ''))
    i_id = str(product.get('itemId', ''))
    v_id = str(product.get('vendorItemId', ''))

    t_p_id = str(target_product_id) if target_product_id else ''
    t_i_id = str(target_item_id) if target_item_id else ''
    t_v_id = str(target_vendor_item_id) if target_vendor_item_id else ''

    # 1ìˆœìœ„: 3ê°œ ëª¨ë‘ ë§¤ì¹­
    if t_p_id and t_i_id and t_v_id:
        if p_id == t_p_id and i_id == t_i_id and v_id == t_v_id:
            return (True, 'full_match')

    # 2ìˆœìœ„: product_id + vendor_item_id
    if t_p_id and t_v_id:
        if p_id == t_p_id and v_id == t_v_id:
            return (True, 'product_vendor')

    # 3ìˆœìœ„: product_id + item_id
    if t_p_id and t_i_id:
        if p_id == t_p_id and i_id == t_i_id:
            return (True, 'product_item')

    # 4ìˆœìœ„: product_idë§Œ
    if t_p_id and p_id == t_p_id:
        return (True, 'product_only')

    # 5ìˆœìœ„: vendor_item_idë§Œ
    if t_v_id and v_id == t_v_id:
        return (True, 'vendor_only')

    # 6ìˆœìœ„: item_idë§Œ
    if t_i_id and i_id == t_i_id:
        return (True, 'item_only')

    return (False, None)


def search_product(query, target_product_id, cookies, tls_profile, proxy,
                   target_item_id=None, target_vendor_item_id=None,
                   max_page=13, verbose=True, save_html=False,
                   total_timeout=20):
    """ìƒí’ˆ ê²€ìƒ‰ (ì ì§„ì  ë°°ì¹˜)

    ë°°ì¹˜ ì „ëµ:
    - Tier 1: 1í˜ì´ì§€ë§Œ (ëŒ€ë¶€ë¶„ ì—¬ê¸°ì„œ ë°œê²¬)
    - Tier 2: 2-5í˜ì´ì§€ ë™ì‹œ (ë¯¸ë°œê²¬ ì‹œ)
    - Tier 3: 6-13í˜ì´ì§€ ë™ì‹œ (ë¯¸ë°œê²¬ ì‹œ)

    ë§¤ì¹­ ìš°ì„ ìˆœìœ„:
    1. product_id + item_id + vendor_item_id (ì™„ì „ ë§¤ì¹­)
    2. product_id + vendor_item_id
    3. product_id + item_id
    4. product_idë§Œ
    5. vendor_item_idë§Œ
    6. item_idë§Œ

    Args:
        query: ê²€ìƒ‰ì–´
        target_product_id: íƒ€ê²Ÿ ìƒí’ˆ ID
        cookies: ì¿ í‚¤ ë”•ì…”ë„ˆë¦¬
        tls_profile: TLS í”„ë¡œí•„ ë ˆì½”ë“œ (tls_profiles í…Œì´ë¸”)
        proxy: í”„ë¡ì‹œ URL
        target_item_id: íƒ€ê²Ÿ ì•„ì´í…œ ID (ì„ íƒ)
        target_vendor_item_id: íƒ€ê²Ÿ ë²¤ë” ì•„ì´í…œ ID (ì„ íƒ)
        max_page: ìµœëŒ€ í˜ì´ì§€
        verbose: ìƒì„¸ ì¶œë ¥
        save_html: HTML ì €ì¥ ì—¬ë¶€ (ìŠ¤í¬ë¦°ìƒ·ìš©)
        total_timeout: ì „ì²´ íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ 20ì´ˆ)

    Returns:
        dict: {
            found: ë°œê²¬ ìƒí’ˆ ì •ë³´ ë˜ëŠ” None,
            id_match_type: ë§¤ì¹­ íƒ€ì… (full_match, product_vendor, product_item, product_only, vendor_only, item_only),
            all_products: ëª¨ë“  ìƒí’ˆ ë¦¬ìŠ¤íŠ¸,
            blocked: ì°¨ë‹¨ ì—¬ë¶€,
            block_error: ì°¨ë‹¨ ì—ëŸ¬ ë©”ì‹œì§€,
            total_bytes: ì´ íŠ¸ë˜í”½,
            response_cookies: ì‘ë‹µ ì¿ í‚¤,
            response_cookies_full: ì‘ë‹µ ì¿ í‚¤ (ì „ì²´ ì†ì„±),
            found_html: ìƒí’ˆ ë°œê²¬ í˜ì´ì§€ HTML (save_html=Trueì¸ ê²½ìš°)
        }
    """
    import time
    start_time = time.time()

    trace_id = generate_trace_id()
    if verbose:
        print(f"\n[{timestamp()}] ê²€ìƒ‰ ì¤‘... (traceId: {trace_id})")

    found = None
    found_html = None  # ìƒí’ˆ ë°œê²¬ í˜ì´ì§€ HTML
    id_match_type = None  # ë§¤ì¹­ íƒ€ì…
    all_products = []
    blocked = False
    block_error = ''
    page_errors = []  # ê° í˜ì´ì§€ë³„ ì—ëŸ¬ ìˆ˜ì§‘
    page_counts = {}  # í˜ì´ì§€ë³„ ìƒí’ˆ ìˆ˜ {1: 72, 2: 72, ...}
    total_bytes = 0
    all_response_cookies = {}
    all_response_cookies_full = []
    pages_searched = 0  # ì„±ê³µì ìœ¼ë¡œ ê²€ìƒ‰í•œ í˜ì´ì§€ ìˆ˜

    # ë°°ì¹˜ ì •ì˜ (ë¹ˆ ë°°ì¹˜ ì œì™¸)
    batches = [
        [1],                    # Tier 1: 1í˜ì´ì§€ ë‹¨ë…
        [2, 3, 4, 5],           # Tier 2: 2-5í˜ì´ì§€ ë™ì‹œ
        list(range(6, min(max_page + 1, 14)))  # Tier 3: 6-13í˜ì´ì§€ ë™ì‹œ
    ]
    batches = [b for b in batches if b]  # ë¹ˆ ë°°ì¹˜ ì œê±°

    cookies_ref = cookies.copy()  # ì¿ í‚¤ ì—…ë°ì´íŠ¸ìš©

    # ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ í”Œë˜ê·¸ (1í˜ì´ì§€ 0ê°œë©´ ì¡°ê¸° ì¢…ë£Œ)
    no_results = False

    for batch_idx, pages in enumerate(batches):
        if found or blocked or no_results:
            break

        # ì „ì²´ íƒ€ì„ì•„ì›ƒ ì²´í¬
        elapsed = time.time() - start_time
        if elapsed >= total_timeout:
            blocked = True
            block_error = f'TOTAL_TIMEOUT_{int(elapsed)}s'
            if verbose:
                print(f"  ğŸ›‘ ì „ì²´ íƒ€ì„ì•„ì›ƒ ({int(elapsed)}ì´ˆ) â†’ ì¡°ê¸° ì¢…ë£Œ")
            break

        if verbose:
            tier_name = ['1í˜ì´ì§€', '2-5í˜ì´ì§€', f'6-{max_page}í˜ì´ì§€'][batch_idx]
            print(f"  Tier {batch_idx + 1}: {tier_name}")

        with ThreadPoolExecutor(max_workers=len(pages)) as executor:
            futures = {
                executor.submit(
                    fetch_page, p, query, trace_id, cookies_ref, tls_profile, proxy, save_html
                ): p for p in pages
            }

            for future in as_completed(futures):
                result = future.result()

                # ì‘ë‹µ ì¿ í‚¤ ìˆ˜ì§‘ (í•„ìˆ˜)
                all_response_cookies.update(result['response_cookies'])
                cookies_ref.update(result['response_cookies'])  # ë‹¤ìŒ ìš”ì²­ì— ë°˜ì˜
                all_response_cookies_full.extend(result['response_cookies_full'])

                total_bytes += result.get('size', 0)

                if result['success']:
                    pages_searched = max(pages_searched, result['page'])  # ìµœëŒ€ í˜ì´ì§€ ì¶”ì 
                    # í˜ì´ì§€ë³„ (ìƒí’ˆ ìˆ˜, ì¬ì‹œë„ íšŸìˆ˜)
                    retried = result.get('retried', 0)
                    page_counts[result['page']] = (len(result['products']), retried)

                    for product in result['products']:
                        product['_page'] = result['page']
                        all_products.append(product)

                        if not found:
                            matched, match_type = _match_product(
                                product, target_product_id, target_item_id, target_vendor_item_id
                            )
                            if matched:
                                found = product
                                found['page'] = result['page']
                                id_match_type = match_type
                                # ìŠ¤í¬ë¦°ìƒ·ìš© HTML ì €ì¥
                                if save_html and result.get('html'):
                                    found_html = result['html']
                                if verbose:
                                    print(f"  [{timestamp()}] âœ… ë°œê²¬! Page {result['page']}, Rank {product['rank']} ({match_type})")
                                # ìƒí’ˆ ë°œê²¬ ì‹œ í˜„ì¬ ë°°ì¹˜ì˜ ë‚˜ë¨¸ì§€ ìš”ì²­ ì·¨ì†Œ ë° ë£¨í”„ ì¢…ë£Œ
                                for f in futures:
                                    f.cancel()
                                break  # for product ë£¨í”„ ì¢…ë£Œ

                    # ìƒí’ˆ ë°œê²¬ ì‹œ for future ë£¨í”„ ì¢…ë£Œ
                    if found:
                        break

                    if verbose:
                        retry_info = f" (retry:{result['retried']})" if result.get('retried', 0) > 0 else ""
                        print(f"    Page {result['page']:2d}: {len(result['products'])}ê°œ{retry_info}")
                else:
                    error = result.get('error', '')
                    retried = result.get('retried', 0)
                    page_counts[result['page']] = (-1, retried)  # ì—ëŸ¬ í˜ì´ì§€ëŠ” -1ë¡œ í‘œì‹œ
                    if error:
                        page_errors.append({'page': result['page'], 'error': error, 'retried': result.get('retried', 0)})
                    if verbose:
                        retry_info = f" (retry:{result.get('retried', 0)})" if result.get('retried', 0) > 0 else ""
                        print(f"    Page {result['page']:2d}: âŒ {error}{retry_info}")

                    # HTTP/2 ìŠ¤íŠ¸ë¦¼ ì—ëŸ¬ë„ ì°¨ë‹¨ìœ¼ë¡œ ì²˜ë¦¬ (ê°€ì¥ í”í•œ ì°¨ë‹¨ ë°©ì‹)
                    if (error == 'BLOCKED_403' or
                        error.startswith('CHALLENGE_') or
                        'HTTP/2 stream' in error or
                        'curl: (92)' in error):
                        blocked = True
                        if 'HTTP/2 stream' in error or 'curl: (92)' in error:
                            block_error = 'HTTP2_PROTOCOL_ERROR'
                        else:
                            block_error = error
                        for f in futures:
                            f.cancel()
                        break

                    # 1í˜ì´ì§€ íƒ€ì„ì•„ì›ƒ ì‹œ ì¡°ê¸° ì¢…ë£Œ (í”„ë¡ì‹œ ë¬¸ì œ)
                    if (batch_idx == 0 and result['page'] == 1 and
                        ('timed out' in error.lower() or 'curl: (28)' in error or
                         'Could not connect' in error or 'curl: (7)' in error)):
                        blocked = True
                        block_error = 'PROXY_TIMEOUT'
                        if verbose:
                            print(f"  ğŸ›‘ 1í˜ì´ì§€ íƒ€ì„ì•„ì›ƒ â†’ ì¡°ê¸° ì¢…ë£Œ")
                        for f in futures:
                            f.cancel()
                        break

        # ë°°ì¹˜ ì™„ë£Œ í›„ ì‹¤íŒ¨í•œ í˜ì´ì§€ ì¬ì‹œë„ (found/blockedê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
        if not found and not blocked:
            # íƒ€ì„ì•„ì›ƒ ì²´í¬
            elapsed = time.time() - start_time
            if elapsed >= total_timeout:
                blocked = True
                block_error = f'TOTAL_TIMEOUT_{int(elapsed)}s'
                if verbose:
                    print(f"  ğŸ›‘ ì „ì²´ íƒ€ì„ì•„ì›ƒ ({int(elapsed)}ì´ˆ) â†’ ì¬ì‹œë„ ìƒëµ")
            else:
                # í˜„ì¬ ë°°ì¹˜ì—ì„œ ì‹¤íŒ¨í•œ í˜ì´ì§€ ì°¾ê¸° (page_countsì—ì„œ -1ì¸ í˜ì´ì§€)
                failed_pages = [p for p in pages if page_counts.get(p, (0, 0))[0] == -1]

                if failed_pages and verbose:
                    print(f"  ğŸ”„ ì‹¤íŒ¨ í˜ì´ì§€ ì¬ì‹œë„: {failed_pages}")

                # ì‹¤íŒ¨ í˜ì´ì§€ ìˆœì°¨ì ìœ¼ë¡œ ì¬ì‹œë„ (1ë²ˆì”©)
                for retry_page in failed_pages:
                    # ì¬ì‹œë„ ì „ íƒ€ì„ì•„ì›ƒ ì²´í¬
                    elapsed = time.time() - start_time
                    if found or blocked or elapsed >= total_timeout:
                        if elapsed >= total_timeout:
                            blocked = True
                            block_error = f'TOTAL_TIMEOUT_{int(elapsed)}s'
                        break

                    result = fetch_page(retry_page, query, trace_id, cookies_ref, tls_profile, proxy, save_html, max_retries=1)

                    # ì‘ë‹µ ì¿ í‚¤ ìˆ˜ì§‘
                    all_response_cookies.update(result['response_cookies'])
                    cookies_ref.update(result['response_cookies'])
                    all_response_cookies_full.extend(result['response_cookies_full'])
                    total_bytes += result.get('size', 0)

                    if result['success']:
                        pages_searched = max(pages_searched, result['page'])
                        retried = result.get('retried', 0) + page_counts.get(retry_page, (0, 0))[1] + 1  # ê¸°ì¡´ ì¬ì‹œë„ + ë°°ì¹˜ ì¬ì‹œë„
                        page_counts[result['page']] = (len(result['products']), retried)

                        for product in result['products']:
                            product['_page'] = result['page']
                            all_products.append(product)

                            if not found:
                                matched, match_type = _match_product(
                                    product, target_product_id, target_item_id, target_vendor_item_id
                                )
                                if matched:
                                    found = product
                                    found['page'] = result['page']
                                    id_match_type = match_type
                                    if save_html and result.get('html'):
                                        found_html = result['html']
                                    if verbose:
                                        print(f"  [{timestamp()}] âœ… ë°œê²¬! Page {result['page']}, Rank {product['rank']} ({match_type}) [ì¬ì‹œë„]")
                                    break

                        if verbose and not found:
                            print(f"    Page {result['page']:2d}: {len(result['products'])}ê°œ (ì¬ì‹œë„ ì„±ê³µ)")
                    else:
                        # ì¬ì‹œë„ë„ ì‹¤íŒ¨ - ê¸°ì¡´ ì—ëŸ¬ ìœ ì§€, ì¬ì‹œë„ íšŸìˆ˜ë§Œ ì—…ë°ì´íŠ¸
                        prev_retried = page_counts.get(retry_page, (0, 0))[1]
                        page_counts[retry_page] = (-1, prev_retried + 1)
                        if verbose:
                            print(f"    Page {retry_page:2d}: âŒ ì¬ì‹œë„ ì‹¤íŒ¨")

        # Tier 1 ì™„ë£Œ í›„ ê²€ìƒ‰ ê²°ê³¼ê°€ 0ê°œë©´ ì¡°ê¸° ì¢…ë£Œ
        # no_resultsëŠ” ì¿ íŒ¡ì´ ëª…ì‹œì ìœ¼ë¡œ "ê²€ìƒ‰ê²°ê³¼ ì—†ìŒ"ì„ ë°˜í™˜í•œ ê²½ìš°ì—ë§Œ True
        # ì—ëŸ¬(íƒ€ì„ì•„ì›ƒ ë“±)ë¡œ ì¸í•œ 0ê°œëŠ” no_results = False
        if batch_idx == 0 and len(all_products) == 0 and not blocked:
            # ì—ëŸ¬ ì—†ì´ ì„±ê³µí•œ ìš”ì²­ ì¤‘ "ê²€ìƒ‰ê²°ê³¼ ì—†ìŒ" í˜ì´ì§€ê°€ ìˆëŠ”ì§€ í™•ì¸
            is_coupang_no_results = any(
                f.result().get('is_no_results_page', False)
                for f in futures if f.done() and not f.cancelled() and f.result().get('success')
            )

            if is_coupang_no_results:
                no_results = True  # ì¿ íŒ¡ì´ ëª…ì‹œì ìœ¼ë¡œ ê²€ìƒ‰ê²°ê³¼ ì—†ìŒ ë°˜í™˜
                if verbose:
                    print(f"  âš ï¸ ì¿ íŒ¡ ê²€ìƒ‰ê²°ê³¼ ì—†ìŒ - ì¡°ê¸° ì¢…ë£Œ")
            else:
                no_results = False  # ì—ëŸ¬ë¡œ ì¸í•œ 0ê°œ (íƒ€ì„ì•„ì›ƒ ë“±)
                if verbose:
                    print(f"  âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì¡°ê¸° ì¢…ë£Œ")

    # ì‹¤ì œ ìˆœìœ„ ê³„ì‚°
    all_products.sort(key=lambda p: (p['_page'], p.get('rank') or 999))
    for i, product in enumerate(all_products):
        product['actual_rank'] = i + 1

    actual_rank = None
    if found:
        # found ìƒí’ˆì˜ actual_rank ì°¾ê¸° (uniqueKeyë¡œ ë§¤ì¹­)
        found_key = found.get('uniqueKey')
        for product in all_products:
            if product.get('uniqueKey') == found_key:
                actual_rank = product['actual_rank']
                break

    # í˜ì´ì§€ë³„ ìƒí’ˆ ìˆ˜ ì •ë ¬ (1í˜ì´ì§€ë¶€í„°)
    sorted_page_counts = dict(sorted(page_counts.items()))

    # ì—ëŸ¬ í˜ì´ì§€(-1)ê°€ 1ê°œë¼ë„ ìˆìœ¼ë©´ blocked ì²˜ë¦¬ (ìƒí’ˆ ë¯¸ë°œê²¬ ì‹œì—ë§Œ)
    # ì™„ì„±ë„ë¥¼ ë§ì¶”ì§€ ëª»í•˜ë©´ ì‹¤íŒ¨ - ì—ëŸ¬ í˜ì´ì§€ì— ìƒí’ˆì´ ìˆì—ˆì„ ìˆ˜ ìˆìŒ
    # page_counts ê°’ì€ (count, retried) íŠœí”Œ
    # count: 0 = ì •ìƒ ì‘ë‹µì´ì§€ë§Œ ìƒí’ˆ ì—†ìŒ, -1 = ì—ëŸ¬
    if not found and not blocked and page_counts:
        error_pages = sum(1 for v in page_counts.values() if v[0] == -1)
        total_pages = len(page_counts)
        if error_pages > 0:
            blocked = True
            block_error = f'INCOMPLETE_{error_pages}/{total_pages}'

    # íŠœí”Œì„ ë¬¸ìì—´ë¡œ ë³€í™˜: (63, 0) -> "63", (63, 2) -> "63(r2)", (-1, 1) -> "-1(r1)"
    page_counts_str = {}
    for page, (count, retried) in sorted_page_counts.items():
        if retried > 0:
            page_counts_str[page] = f"{count}(r{retried})"
        else:
            page_counts_str[page] = str(count)

    return {
        'found': found,
        'actual_rank': actual_rank,
        'id_match_type': id_match_type,  # ë§¤ì¹­ íƒ€ì… ì¶”ê°€
        'all_products': all_products,
        'blocked': blocked,
        'block_error': block_error,
        'page_errors': page_errors,
        'page_counts': page_counts_str,  # í˜ì´ì§€ë³„ ìƒí’ˆ ìˆ˜ {1: "72", 2: "72(r1)", 13: "-1(r2)"}
        'total_bytes': total_bytes,
        'trace_id': trace_id,
        'response_cookies': all_response_cookies,
        'response_cookies_full': all_response_cookies_full,
        'found_html': found_html,
        'no_results': no_results,  # ì¿ íŒ¡ì´ "ê²€ìƒ‰ê²°ê³¼ ì—†ìŒ" ì‘ë‹µ (ì •ìƒì ì¸ ë¯¸ë°œê²¬)
        'pages_searched': pages_searched  # ì„±ê³µì ìœ¼ë¡œ ê²€ìƒ‰ ì™„ë£Œí•œ ìµœëŒ€ í˜ì´ì§€
    }


if __name__ == '__main__':
    print("ê²€ìƒ‰ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print("ì‚¬ìš©ë²•: search.pyëŠ” ì§ì ‘ ì‹¤í–‰í•˜ì§€ ì•Šê³  ëª¨ë“ˆë¡œ importí•˜ì—¬ ì‚¬ìš©")
